{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d50913f8-092e-4dff-9fdf-c1f231fe0f27",
   "metadata": {},
   "source": [
    "# LDA and PCA\n",
    "\n",
    "In today's review session, we are going over running Latent Dirichlet Allocation and PCA on document vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dc34ac-3c8f-4a0f-9964-e2a413f3edf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before we start, we load up the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b298f4-0c2c-4f57-bb34-0111766c6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from statsmodels.multivariate.pca import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd45742d-5c54-438b-bcb0-483c6b61b03c",
   "metadata": {},
   "source": [
    "We will be working with the New York Times headlines. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f59fb8b-5d19-4453-a856-649e41278872",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'Your key'\n",
    "nyt_headlines = []\n",
    "nyt_abstract = []\n",
    "nyt_sections = []\n",
    "nyt_dates = []\n",
    "year = 2019\n",
    "for month in [1, 2, 3]:\n",
    "    response = requests.get(f'https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={api_key}')\n",
    "    content = response.json()\n",
    "    for article in content['response']['docs']:\n",
    "        nyt_headlines.append(article['headline']['main'])\n",
    "        nyt_sections.append(article['section_name'])\n",
    "        nyt_abstract.append(article['abstract'])\n",
    "        nyt_dates.append(article['pub_date'])\n",
    "nyt_df = pd.DataFrame({'headline': nyt_headlines, 'abstract': nyt_abstract, 'date': nyt_dates, 'section': nyt_sections})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a26876-9479-46ff-8e3c-6cb24a41276c",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation\n",
    "\n",
    "We will first go over how Latent Dirichlet Allocation works in python. What LDA does is essentially assuming a data-generating process for topics, documents, and words. Observing the documents, LDA backs out the probability distribution over words for a given topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29124f42-0040-4f04-b80a-3189da07f32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(nyt_df['headline'])\n",
    "lda = LatentDirichletAllocation(30, random_state=1680).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6422c04f-fec8-41ea-9d73-41e1ea40307f",
   "metadata": {},
   "source": [
    "Note that we are keeping the headline vectors in term frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d95665e-d06a-461f-99a3-6b42768940a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: quiz, word, climate, change, economy, strong, texas, review, williams, face\n",
      "Topic 1: trump, kim, college, scandal, film, north, admissions, korea, family, weekend\n",
      "Topic 2: year, white, super, bowl, game, say, women, school, old, history\n",
      "Topic 3: crisis, review, venezuela, stone, roger, border, pelosi, nancy, leaders, china\n",
      "Topic 4: need, judge, new, brazil, strike, students, dies, teachers, john, workers\n",
      "Topic 5: art, trump, season, weekend, true, save, view, dance, american, scene\n",
      "Topic 6: best, says, 11, rights, reads, iran, dies, worst, civil, changed\n",
      "Topic 7: el, years, chapo, trial, accused, health, care, abuse, faces, abortion\n",
      "Topic 8: know, smollett, jussie, sexual, homes, sale, review, law, like, connecticut\n",
      "Topic 9: 2019, march, corrections, going, february, letter, year, women, money, home\n",
      "Topic 10: russia, saudi, trump, netflix, bezos, new, jeff, justice, says, arabia\n",
      "Topic 11: golden, globes, house, return, review, 30, hunting, john, island, russian\n",
      "Topic 12: report, 2020, mueller, war, michael, help, million, jackson, media, work\n",
      "Topic 13: weekend, brooklyn, shows, federal, music, concerts, cold, really, pop, jail\n",
      "Topic 14: briefing, brexit, tv, friday, thursday, wednesday, tuesday, dealbook, evening, monday\n",
      "Topic 15: trump, border, shutdown, wall, state, emergency, union, government, president, national\n",
      "Topic 16: new, zealand, today, attacks, review, mayor, attack, opera, shooting, victims\n",
      "Topic 17: finds, trump, deal, review, democrats, women, weather, abrams, china, case\n",
      "Topic 18: week, court, politics, new, trump, cohen, supreme, books, public, green\n",
      "Topic 19: life, want, making, women, high, think, pay, school, don, heart\n",
      "Topic 20: 000, right, cook, william, barr, homes, sold, book, oscar, marvel\n",
      "Topic 21: trump, says, dies, official, act, gets, big, love, review, health\n",
      "Topic 22: new, york, city, times, market, boeing, max, news, 737, week\n",
      "Topic 23: america, man, party, taliban, peace, afghan, afghanistan, killing, police, carlos\n",
      "Topic 24: harris, kamala, live, ilhan, omar, northam, virginia, building, photo, canadian\n",
      "Topic 25: time, living, crash, ethiopian, airlines, watching, grammys, weekend, cover, review\n",
      "Topic 26: real, new, elizabeth, warren, estate, win, wants, robert, guilty, kraft\n",
      "Topic 27: day, quotation, china, run, good, trade, running, deal, end, trump\n",
      "Topic 28: things, isn, kids, los, angeles, hours, stay, self, mets, 36\n",
      "Topic 29: 2019, fall, oscars, january, corrections, 10, san, review, red, picture\n"
     ]
    }
   ],
   "source": [
    "topic_words = {}\n",
    "n_top_words = 10\n",
    "vocab = vectorizer.get_feature_names()\n",
    "for topic, comp in enumerate(lda.components_):\n",
    "    word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
    "    topic_words[topic] = [vocab[i] for i in word_idx]\n",
    "for topic, words in topic_words.items():\n",
    "    print(f\"Topic {topic}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6099e8a1-75de-4f70-b5b4-d09483f83a10",
   "metadata": {},
   "source": [
    "It is unclear what the topics above really are. It looks like topic 15 is about Trump-related policy, and we can see if this is true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cc70d5d-8f52-4ada-bc5c-ae8e0665e940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two Lives in Art, and a Collection Tracing Their Trajectory',\n",
       " 'Steady as She Glows',\n",
       " 'Automakers Retool Marketing Machines as They Go Electric',\n",
       " 'Trump Laid Out Evidence That a Wall Is Needed. We Took a Hard Look.',\n",
       " 'National Emergency Powers and Trump’s Border Wall, Explained',\n",
       " 'Trump’s Wall of Shame',\n",
       " 'DealBook Briefing: What the State of the Union Means for Business',\n",
       " 'After Falling Under Obama, America’s Uninsured Rate Looks to Be Rising',\n",
       " 'U.A.E. to Use Equipment From Huawei Despite American Pressure',\n",
       " 'A Tipster Pointed to Where a Body Was Buried, Revealing a 40-Year-Old Mystery']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving probability for topic 15 for all headlines\n",
    "loadings = lda.transform(X).T[15, :]\n",
    "# Randomly sample 10 headlines that have loadings over the threshold of 0.3\n",
    "list(nyt_df['headline'][loadings>= 0.3].sample(10, random_state=1680))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350889df-d95e-49b3-acbe-48a527662db2",
   "metadata": {},
   "source": [
    "We can see that there are a few Trump or national security related headlines. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64412f84-13ad-4e99-a6c6-1662b0c9569a",
   "metadata": {},
   "source": [
    "# Principal Component Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d6a554-5df0-4a81-80a5-9baa51f9cf8b",
   "metadata": {},
   "source": [
    "If we want to look at historical headlines, we will have to make multiple requests. Usually, for APIs, there are per minute or per day limits. To avoid reaching the limit, it is good to \"sleep\" between requests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d75f5a07-7b1e-4c56-849f-08f8a876886a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toks = nyt_df['headline'][0:200].tolist()\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(toks)\n",
    "pca = PCA(X.A, ncomp=1, standardize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1903f13f-5fe7-4a7f-930d-37248529988c",
   "metadata": {},
   "source": [
    "The above principal compoenent analysis reduces the dimension of words into 1, which is particularly useful since document vectors are usually very high-dimensional. However, as was the case with PCA with numerical data, we face interpretability issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64acb2d1-9213-41b6-92d3-852e5469be18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         feature  loadings\n",
      "90         barry  0.305857\n",
      "169  christopher  0.305857\n",
      "320        flynn  0.305857\n",
      "357      gillian  0.305857\n",
      "447      jenkins  0.305857\n",
      "452         john  0.305857\n",
      "474    krasinski  0.305857\n",
      "538    mcquarrie  0.305857\n",
      "783       script  0.305857\n",
      "790      secrets  0.305857\n",
      "     feature  loadings\n",
      "400      his -0.057047\n",
      "121     bowl -0.055538\n",
      "355     gift -0.055538\n",
      "360    gives -0.055538\n",
      "547    meyer -0.055538\n",
      "605     ohio -0.055538\n",
      "638  parting -0.055538\n",
      "755     rose -0.055538\n",
      "956    urban -0.055538\n",
      "968  victory -0.055538\n"
     ]
    }
   ],
   "source": [
    "loadings_df = pd.DataFrame({'feature': vectorizer.get_feature_names(), 'loadings': pca.loadings.T[0]})\n",
    "print(loadings_df.nlargest(10, 'loadings'))\n",
    "print(loadings_df.nsmallest(10, 'loadings'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b2410-9660-420e-9b81-29b3bf695fb6",
   "metadata": {},
   "source": [
    "One thing we can do to check what the principal compoenent could mean, is by looking at its loadings on all the words. By looking at the loadings on words, one could get a sense of what type of document would have a high value in a principal component. Here, I printed out 10 words with the largest loadings and 10 words with the smallest loadings. The top 10 words seem to be related to people like John Krasinski, Christopher Mcquarrie, and Gillian Flynn, who are writers and directors, whereas the last few words seem to have to do with Urban Meyer and Ohio State football. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
